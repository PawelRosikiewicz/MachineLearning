{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ed90e26-91f8-4e33-b5b2-7dcc50d124a6",
   "metadata": {},
   "source": [
    "# ENSEMBLE METHODS\n",
    "---\n",
    "All materials presented in that notebooks were created or modified by:   __Pawel Rosikiewicz__ __www.SimpleAI.ch__\n",
    "\n",
    "\n",
    "Note from Author  \n",
    "_I am using these functions on daily basis for EDA in my projects, thus I hope you will find mhem uefull. All fubnctions my functions are avaiable in src folder, under MIT Licence_\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9b1dd2-dfd3-4e8c-b6e3-8b05e4df9186",
   "metadata": {},
   "source": [
    "selected sources used to create my notes\n",
    "* boosting vs bagging; https://quantdare.com/what-is-the-difference-between-bagging-and-boosting/\n",
    "* stacking https://quantdare.com/dream-team-combining-classifiers/\n",
    "* Adaboost https://www.analyticsvidhya.com/blog/2021/09/adaboost-algorithm-a-complete-guide-for-beginners/\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16107fd-6454-4b0b-9a37-f71f933998d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2e1e9ea-08a0-4d57-90a2-218665f14622",
   "metadata": {},
   "source": [
    "## Multiclassifiers\n",
    "---   \n",
    "* use several classifiers with a common objective to reduce the variance, and incorporate potential outliers or more difficult cases   \n",
    "* combine multiple models (weak learners) to reach the final output (strong learners).\n",
    "* synonymes_ multi-models, multiple classifier systems, combining classifiers, decision committe,   \n",
    "* roughtly, two big groups:  \n",
    "    * __Ensemble methods__; systems that combine to create a new system using the same learning technique.   \n",
    "        * __Bagging__; also called Bootstrap aggregating, \n",
    "            * random forest; \n",
    "                *  generally outperform decision trees, but their accuracy is lower than gradient boosted trees\n",
    "            * random subspace method - attribute bagging, \n",
    "                * trains N-estimators on randomy selected set of features\n",
    "        * __Boosting__; eg:  \n",
    "            * AdaBoost, LogitBoost, \n",
    "    * __Hybrid methods__; use different learners and combines them using new learning techniques.    \n",
    "        * Stacking (or Stacked Generalization)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4f934b-dbee-4afb-94c9-dea433f6e169",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b8563e-87b9-458e-b1c2-a6609b8f2581",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
