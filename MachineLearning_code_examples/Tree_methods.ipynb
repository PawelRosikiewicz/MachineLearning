{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "caff4059-0d60-4291-b8ec-7d4661ac4d3e",
   "metadata": {},
   "source": [
    "# DECISION TREES & THRIE VISZULALIZATIONS\n",
    "notes and code examples created or collected by Pawel Rosikiewicz, __www.SimpleAI.ch__   \n",
    "part of the text in that text copied from great text of __Nagesh Singh Chauhan__ - link in sourses. Thank you Nagesh.\n",
    "\n",
    "## CONTENT\n",
    "* __Decision trees__\n",
    "    * theory, \n",
    "    * sklearn obejcts,\n",
    "        * DecisionTreeClassifier\n",
    "    * tree vizulalization\n",
    "        * Graphiz package, \n",
    "        * using pandas for better intepretation,\n",
    "        \n",
    "* __sources__\n",
    "    * realy good article on decision trees wiht example in python, sklearn by __Nagesh Singh Chauhan__ https://www.kdnuggets.com/2020/01/decision-tree-algorithm-explained.html - I copied some part of text from his artickle, if you want more info, please chek directly in his work.\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faeb77d5-15fd-4a1f-97a3-cbcf0b7d5eb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d9f3ef-6b75-4fef-9b83-5f0be4f007a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8099a37-2f9e-413f-9992-36098c312762",
   "metadata": {},
   "source": [
    "## MODEL ASSUMPTIONS\n",
    "---\n",
    "1. the whole training set is considered as the root.\n",
    "2. Feature values need to be categorical. If the values are continuous then they are discretized prior to building the model.\n",
    "3. Records are distributed recursively on the basis of attribute values.\n",
    "4. Order to placing attributes as root or internal node of the tree is done by using some statistical approach.\n",
    "\n",
    "from: https://www.kaggle.com/code/prashant111/decision-tree-classifier-tutorial/notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f5be51-c159-4f45-ad64-b9948c72cbb7",
   "metadata": {},
   "source": [
    "![outliers_slide_01](images/DT_slide01.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbfdb5e-b4e8-4026-ae39-8751dd91f5d4",
   "metadata": {},
   "source": [
    "## Prunning\n",
    "copied from: https://www.kaggle.com/code/prashant111/decision-tree-classifier-tutorial/notebook\n",
    "\n",
    "__Overfitting__ is a practical problem while building a Decision-Tree model. The problem of overfitting is considered when the algorithm continues to go deeper and deeper to reduce the training-set error but results with an increased test-set error. So, accuracy of prediction for our model goes down. It generally happens when we build many branches due to outliers and irregularities in data.\n",
    "\n",
    "Two approaches which can be used to avoid overfitting are as follows:-\n",
    "\n",
    "* __Pre-Pruning__\n",
    "    * In pre-pruning, we stop the tree construction a bit early. \n",
    "    * We prefer not to split a node if its goodness measure is below a threshold value. \n",
    "    * it is difficult to choose an appropriate stopping point.\n",
    "\n",
    "* __Post-Pruning__\n",
    "    * In post-pruning, we go deeper and deeper in the tree to build a complete tree. \n",
    "    * If the tree shows the overfitting problem then pruning is done as a post-pruning step. \n",
    "    * We use the __cross-validation data__ to check the effect of our pruning. \n",
    "        * Using cross-validation data, we test whether expanding a node will result in improve or not. \n",
    "        * If it shows an improvement, then we can continue by expanding that node. But if it shows a reduction in accuracy then it should not be expanded. So, the node should be converted to a leaf node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d80ca4-e2b4-4028-aff5-511661d6e3c9",
   "metadata": {},
   "source": [
    "## __Decision trees - Protential problmes, and solutions__ \n",
    "___\n",
    "\n",
    "### __Imbalanced classes, (class Imbalance problem)__\n",
    "* also present when using k-NN\n",
    "* Issue: the tree classifier will optimize toward most frequent class in target variable, \n",
    "* __Solutions:__     \n",
    "    * remove some examples from the most frequent class, \n",
    "    * modify objective function of a classifier/cost function \n",
    "        * eg. add weights to less frequent/underrepresented class/es\n",
    "        * class_weight - it is a parameter in skleanr DecisionTreeClassifier, you need to set it to {balanced}\n",
    "        \n",
    "        \n",
    "### __avoiding overfitting__  \n",
    "* __Prunning__\n",
    "* __apply random forest__, an ensemble method based onn decision trees, \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcccacf0-6e08-4d84-8e25-7abb696b156b",
   "metadata": {},
   "source": [
    "![outliers_slide_01](images/DT_slide02.jpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbcdacd-2099-4b00-a745-99ebd895fd79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a01f4bf-72f8-48ee-bdcb-7c7ebaa7c8fe",
   "metadata": {},
   "source": [
    "## Sklearn implementation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ab0cf0-57f7-4e20-b8af-e3a6210fa06e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6485c8c2-9c38-469d-ae06-a815bbaac8a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4bd701-e781-45f1-a244-f230382e05cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebc7a3c-a681-44bd-9249-c789868f49e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca17450-4586-48c5-929c-cc89c754fb4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307f5080-1272-4111-8a08-e853e1584a2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da3e0a9-6008-4d4b-b345-c5a93279d197",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842f023c-e8e6-4cae-9c39-5802bd1aa250",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fd0399-3984-4a2e-9308-4f8b75ff69bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba212d3f-5537-408c-9fff-829ec2132ab2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a1ff09-1b0f-489f-a5cc-ce469c9ebb4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac5a2bb-8815-4e05-9208-168b2eb2c903",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564f4590-1c81-4991-9c2d-e58dd9d4943d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea1fc04-f2e3-448f-ac3a-b4b47ad50aea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
